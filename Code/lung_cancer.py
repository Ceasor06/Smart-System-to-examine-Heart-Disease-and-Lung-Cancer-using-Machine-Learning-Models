# -*- coding: utf-8 -*-
"""lung_cancer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NAJnREvrAEmb4rrpDtdN7Pbf_YuDwpac
"""

from google.colab import drive

drive.mount('/content/grive')

"""Importing necessary libraries"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from sklearn.metrics import classification_report,confusion_matrix, accuracy_score
import seaborn as sns
from xgboost import XGBClassifier
import matplotlib.pyplot as plt
import os
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn import datasets, linear_model, metrics
from sklearn import svm
# %matplotlib inline
sns.set_theme(color_codes=True, style='darkgrid', 
              palette='deep', font='sans-serif')

df = pd.read_csv("/content/grive/MyDrive/Colab Notebooks/survey lung cancer.csv")
df.head()

df.info()

df.describe()

df.columns

df.isnull().sum()

df.shape

df.duplicated().sum()

df = df.drop_duplicates()

corr = df.corr()
corr

"""Visualisation

"""

sns.distplot(a=df["AGE"]);

sns.boxplot(y = 'AGE', data = df);

sns.countplot(x="LUNG_CANCER", data=df);

r = df.groupby('LUNG_CANCER')['LUNG_CANCER'].count()
plt.pie(r, explode=[0.05, 0.1], labels=['No', 'Yes'], radius=1.5, autopct='%1.1f%%',  shadow=True);

fig, axes = plt.subplots(4, 4, figsize=(25, 15))
fig.suptitle('Different feature distributions')

axes = axes.reshape(16,)

for i,column in enumerate(df.columns):
    sns.histplot(ax = axes[i],data = df, x= column)

plt.figure(figsize=(20,10))
sns.heatmap(df.corr(), annot=True);

df.reset_index()

le = LabelEncoder()

df1 = df.copy(deep=True)

"""**Normalisation**"""

df1.GENDER = le.fit_transform(df1.GENDER)
df1.LUNG_CANCER = le.fit_transform(df1.LUNG_CANCER)

scaler = MinMaxScaler(copy=True, feature_range=(0, 1))
df2 = pd.DataFrame(scaler.fit_transform(df1),columns=df.columns,index=df.index)

X = df2.drop('LUNG_CANCER',axis=1)
y = df2.LUNG_CANCER
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 50)

sm = SMOTE(random_state = 500)
X_res, y_res = sm.fit_resample(X_train, y_train)

"""XGBClassifier"""

model = XGBClassifier(learning_rate=0.2,n_estimators=5000,use_label_encoder=False,random_state=40)
model.fit(X_res, y_res)
y_pred = model.predict(X_test)
accuracy = model.score(X_test, y_test)
accuracy

print(classification_report(y_test,y_pred))

"""KNN"""

knn = KNeighborsClassifier(n_neighbors = 4)
knn.fit(X_train,y_train)
pred = knn.predict(X_test)
print(metrics.accuracy_score(y_test, pred))

"""Logistic Regression"""

logmodel = LogisticRegression()
logmodel.fit(X_train,y_train)
pred = logmodel.predict(X_test)
print(logmodel.score(X_test,y_test))

"""SVM"""

svc = svm.SVC()
svc.fit(X_train,y_train)
score=[]

y_true2 = svc.predict(X_test)
s5 = accuracy_score(y_test,y_true2)
score.append(s5*100)
print(s5)

clf1=LogisticRegression()
clf1.fit(X_train,y_train)
pred1=clf1.predict(X_test)
print(clf1.coef_)
print(clf1.intercept_)
s1=accuracy_score(y_test,pred1)
score.append(s1*100)
print(s1)